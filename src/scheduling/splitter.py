"""
æ­£å¼æ™®é€šå¿—æ„¿è€…å’Œå‚¨å¤‡å¿—æ„¿è€…æ‹†åˆ†ç¨‹åº
ç¨‹åºäºŒï¼šæ ¹æ®é¢è¯•æˆç»©æ‹†åˆ†æ™®é€šå¿—æ„¿è€…

è¾“å…¥ï¼šæ™®é€šå¿—æ„¿è€…è¡¨Excelæ–‡ä»¶ï¼Œmetadata.jsonæ–‡ä»¶ï¼Œé¢è¯•æ±‡æ€»è¡¨Excelæ–‡ä»¶
è¾“å‡ºï¼šæ­£å¼æ™®é€šå¿—æ„¿è€…è¡¨Excelæ–‡ä»¶å’Œå‚¨å¤‡å¿—æ„¿è€…è¡¨Excelæ–‡ä»¶

åŠŸèƒ½ï¼šæ ¹æ®metadata.jsonæ–‡ä»¶ä¸­çš„æ­£å¼æ™®é€šå¿—æ„¿è€…æ€»äººæ•°Mï¼Œå°†æ™®é€šå¿—æ„¿è€…è¡¨æ‹†åˆ†æˆä¸¤å¼ è¡¨ï¼š
- æ­£å¼æ™®é€šå¿—æ„¿è€…è¡¨ï¼ˆé¢è¯•æ±‡æ€»è¡¨ä¸­å½’ä¸€åŒ–æˆç»©æ’åºçš„å‰Måï¼‰
- å‚¨å¤‡å¿—æ„¿è€…è¡¨ï¼ˆåé¢çš„äººå‘˜ï¼‰ï¼ŒæŒ‰é¢è¯•æˆç»©ä»é«˜åˆ°ä½æ’åˆ—
"""

import os
import sys
import json
from pathlib import Path
from typing import Dict, List, Tuple, Any
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.utils.logger_factory import get_logger
from src.utils._excel_handler import ExcelHandler
from config.loader import CONFIG


class VolunteerSplitter:
    """å¿—æ„¿è€…æ‹†åˆ†å™¨"""

    def __init__(self):
        self.logger = get_logger(__file__)
        self.handler = ExcelHandler()

        # é…ç½®è·¯å¾„
        self.input_dir = CONFIG.get('paths.input_dir')
        self.interview_results_dir = CONFIG.get('paths.interview_results_dir')
        self.scheduling_prep_dir = CONFIG.get('paths.scheduling_prep_dir')

    def run_split(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¿—æ„¿è€…æ‹†åˆ†æµç¨‹"""
        self.logger.info("å¼€å§‹æ‰§è¡Œæ­£å¼æ™®é€šå¿—æ„¿è€…å’Œå‚¨å¤‡å¿—æ„¿è€…æ‹†åˆ†")

        results = {
            'formal_volunteers_file': None,
            'backup_volunteers_file': None,
            'formal_count': 0,
            'backup_count': 0,
            'errors': [],
            'warnings': []
        }

        try:
            # æ­¥éª¤1ï¼šè¯»å–è¾“å…¥æ–‡ä»¶
            normal_volunteers_df, metadata, interview_scores_df = self._read_input_files()

            # æ­¥éª¤2ï¼šè·å–æ­£å¼å¿—æ„¿è€…äººæ•°M
            formal_count = self._get_formal_volunteer_count(metadata)

            # æ­¥éª¤3ï¼šæ ¹æ®é¢è¯•æˆç»©æ’åºå’Œæ‹†åˆ†
            formal_df, backup_df = self._split_by_interview_scores(
                normal_volunteers_df, interview_scores_df, formal_count
            )

            # æ­¥éª¤4ï¼šä¿å­˜æ‹†åˆ†ç»“æœ
            formal_file = self._save_formal_volunteers(formal_df)
            backup_file = self._save_backup_volunteers(backup_df)

            # æ­¥éª¤5ï¼šæ›´æ–°å…ƒæ•°æ®
            self._update_metadata(metadata, len(formal_df), len(backup_df))

            results.update({
                'formal_volunteers_file': formal_file,
                'backup_volunteers_file': backup_file,
                'formal_count': len(formal_df),
                'backup_count': len(backup_df)
            })

            self.logger.info(f"æ‹†åˆ†å®Œæˆï¼šæ­£å¼å¿—æ„¿è€… {len(formal_df)} äººï¼Œå‚¨å¤‡å¿—æ„¿è€… {len(backup_df)} äºº")

        except Exception as e:
            self.logger.error(f"å¿—æ„¿è€…æ‹†åˆ†å¤±è´¥: {str(e)}")
            results['errors'].append(str(e))

        return results

    def _read_input_files(self) -> Tuple[pd.DataFrame, Dict, pd.DataFrame]:
        """è¯»å–è¾“å…¥æ–‡ä»¶"""
        self.logger.info("è¯»å–è¾“å…¥æ–‡ä»¶")

        # è¯»å–æ™®é€šå¿—æ„¿è€…è¡¨
        normal_file = os.path.join(self.interview_results_dir, CONFIG.get('files.normal_volunteers'))
        if not os.path.exists(normal_file):
            raise FileNotFoundError(f"æ™®é€šå¿—æ„¿è€…è¡¨ä¸å­˜åœ¨: {normal_file}")

        normal_df = self.handler.read_excel(normal_file)
        self.logger.info(f"è¯»å–æ™®é€šå¿—æ„¿è€…è¡¨: {len(normal_df)} è¡Œ")

        # è¯»å–å…ƒæ•°æ®æ–‡ä»¶
        metadata_file = os.path.join(self.scheduling_prep_dir, CONFIG.get('files.metadata'))
        if not os.path.exists(metadata_file):
            raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")

        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        self.logger.info("è¯»å–å…ƒæ•°æ®æ–‡ä»¶")

        # è¯»å–é¢è¯•æ±‡æ€»è¡¨
        interview_file = os.path.join(self.interview_results_dir, CONFIG.get('files.unified_interview_scores'))
        if not os.path.exists(interview_file):
            raise FileNotFoundError(f"é¢è¯•æ±‡æ€»è¡¨ä¸å­˜åœ¨: {interview_file}")

        interview_df = self.handler.read_excel(interview_file)
        self.logger.info(f"è¯»å–é¢è¯•æ±‡æ€»è¡¨: {len(interview_df)} è¡Œ")

        return normal_df, metadata, interview_df

    def _get_formal_volunteer_count(self, metadata: Dict) -> int:
        """ä»å…ƒæ•°æ®ä¸­è·å–æ­£å¼å¿—æ„¿è€…äººæ•°"""
        # é¦–å…ˆå°è¯•ä»å…ƒæ•°æ®ç»Ÿè®¡ä¸­è·å–
        stats = metadata.get('statistics', {})
        formal_count = stats.get('formal_normal_count')

        if formal_count is not None and formal_count > 0:
            self.logger.info(f"ä»å…ƒæ•°æ®è·å–æ­£å¼å¿—æ„¿è€…äººæ•°: {formal_count}")
            return formal_count

        # å¦‚æœå…ƒæ•°æ®ä¸­æ²¡æœ‰ï¼Œåˆ™æ ¹æ®æ€»éœ€æ±‚äººæ•°ä¼°ç®—
        total_required = stats.get('total_required_volunteers', 0)
        internal_count = stats.get('internal_volunteer_count', 0)
        family_count = stats.get('family_volunteer_count', 0)
        group_count = stats.get('group_volunteer_count', 0)

        # è®¡ç®—éœ€è¦çš„æ™®é€šå¿—æ„¿è€…äººæ•°
        needed_normal = max(0, total_required - internal_count - family_count - group_count)

        # æ€»æ™®é€šå¿—æ„¿è€…äººæ•°
        total_normal = stats.get('normal_volunteer_total', 0)

        # æ­£å¼å¿—æ„¿è€…äººæ•°å–éœ€è¦äººæ•°å’Œæ€»äººæ•°çš„è¾ƒå°å€¼
        formal_count = min(needed_normal, total_normal)

        self.logger.info(f"æ ¹æ®éœ€æ±‚ä¼°ç®—æ­£å¼å¿—æ„¿è€…äººæ•°: {formal_count} (éœ€è¦: {needed_normal}, æ€»æ•°: {total_normal})")

        if formal_count <= 0:
            raise ValueError("æ— æ³•ç¡®å®šæ­£å¼å¿—æ„¿è€…äººæ•°ï¼Œè¯·æ£€æŸ¥å…ƒæ•°æ®é…ç½®")

        return formal_count

    def _split_by_interview_scores(self, normal_df: pd.DataFrame, interview_df: pd.DataFrame,
                                  formal_count: int) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """æ ¹æ®é¢è¯•æˆç»©æ‹†åˆ†å¿—æ„¿è€…"""
        self.logger.info(f"æ ¹æ®é¢è¯•æˆç»©æ‹†åˆ†å¿—æ„¿è€…ï¼Œæ­£å¼å¿—æ„¿è€…äººæ•°: {formal_count}")

        # ä½¿ç”¨ExcelHandlerçš„æ¨¡ç³ŠåŒ¹é…åŠŸèƒ½æŸ¥æ‰¾å­¦å·åˆ—
        field_mappings = CONFIG.get('field_mappings', {})
        student_id_keyword = field_mappings.get('student_id', 'å­¦å·')

        # åœ¨æ™®é€šå¿—æ„¿è€…è¡¨ä¸­æŸ¥æ‰¾å­¦å·åˆ—
        normal_mapping = self.handler.find_columns_by_keywords(normal_df, {
            'student_id': student_id_keyword
        })

        # åœ¨é¢è¯•æ±‡æ€»è¡¨ä¸­æŸ¥æ‰¾å­¦å·åˆ—
        interview_mapping = self.handler.find_columns_by_keywords(interview_df, {
            'student_id': student_id_keyword
        })

        if not normal_mapping:
            raise ValueError(f"æ™®é€šå¿—æ„¿è€…è¡¨ä¸­æœªæ‰¾åˆ°å­¦å·åˆ— (æœç´¢å…³é”®è¯: {student_id_keyword})")

        if not interview_mapping:
            raise ValueError(f"é¢è¯•æ±‡æ€»è¡¨ä¸­æœªæ‰¾åˆ°å­¦å·åˆ— (æœç´¢å…³é”®è¯: {student_id_keyword})")

        # è·å–å®é™…åˆ—å
        normal_student_id_col = list(normal_mapping.keys())[0]
        interview_student_id_col = list(interview_mapping.keys())[0]

        self.logger.debug(f"æ™®é€šå¿—æ„¿è€…è¡¨å­¦å·åˆ—: {normal_student_id_col}")
        self.logger.debug(f"é¢è¯•æ±‡æ€»è¡¨å­¦å·åˆ—: {interview_student_id_col}")

        # æ ‡å‡†åŒ–å­¦å·åˆ—åä¸º'å­¦å·'
        normal_df = normal_df.rename(columns={normal_student_id_col: 'å­¦å·'})
        interview_df = interview_df.rename(columns={interview_student_id_col: 'å­¦å·'})

        # æ£€æŸ¥æˆç»©åˆ—
        score_column = None
        possible_score_columns = ['å½’ä¸€åŒ–æˆç»©', 'normalized_score', 'æˆç»©', 'score']

        for col in possible_score_columns:
            if col in interview_df.columns:
                score_column = col
                break

        if score_column is None:
            self.logger.warning("é¢è¯•æ±‡æ€»è¡¨ä¸­æœªæ‰¾åˆ°æˆç»©åˆ—ï¼Œä½¿ç”¨åŸå§‹é¡ºåº")
            # æ²¡æœ‰æˆç»©åˆ—ï¼Œä½¿ç”¨åŸå§‹é¡ºåº
            if len(normal_df) <= formal_count:
                return normal_df.copy(), pd.DataFrame()
            else:
                formal_df = normal_df.iloc[:formal_count].copy()
                backup_df = normal_df.iloc[formal_count:].copy()
                return formal_df, backup_df

        # åˆå¹¶æ™®é€šå¿—æ„¿è€…è¡¨å’Œé¢è¯•æˆç»©
        merged_df = normal_df.merge(
            interview_df[['å­¦å·', score_column]],
            on='å­¦å·',
            how='left'
        )

        # æ£€æŸ¥åˆå¹¶ç»“æœ
        missing_scores = merged_df[score_column].isna().sum()
        if missing_scores > 0:
            self.logger.warning(f"æœ‰ {missing_scores} ä¸ªå¿—æ„¿è€…ç¼ºå°‘é¢è¯•æˆç»©")
            # å°†ç¼ºå°‘æˆç»©çš„æˆç»©è®¾ä¸º-1ï¼Œæ’åˆ°æœ€å
            merged_df[score_column] = merged_df[score_column].fillna(-1)

        # æŒ‰æˆç»©æ’åºï¼ˆé™åºï¼‰- å…¼å®¹ä¸åŒç‰ˆæœ¬çš„pandas
        try:
            # å°è¯•ä½¿ç”¨ na_last å‚æ•°ï¼ˆè¾ƒæ–°ç‰ˆæœ¬çš„pandasï¼‰
            merged_df = merged_df.sort_values(by=score_column, ascending=False, na_last=True)
        except TypeError:
            # å¦‚æœä¸æ”¯æŒ na_last å‚æ•°ï¼Œåˆ™å…ˆå¤„ç†NaNå€¼å†æ’åº
            merged_df = merged_df.fillna({score_column: -1})  # å°†NaNè®¾ä¸º-1
            merged_df = merged_df.sort_values(by=score_column, ascending=False)

        # æ‹†åˆ†
        if len(merged_df) <= formal_count:
            # æ€»äººæ•°ä¸è¶…è¿‡æ­£å¼å¿—æ„¿è€…äººæ•°ï¼Œå…¨éƒ¨ä¸ºæ­£å¼å¿—æ„¿è€…
            formal_df = merged_df.copy()
            backup_df = pd.DataFrame()
            self.logger.info("æ™®é€šå¿—æ„¿è€…æ€»æ•°ä¸è¶…è¿‡æ­£å¼å¿—æ„¿è€…äººæ•°ï¼Œå…¨éƒ¨ä¸ºæ­£å¼å¿—æ„¿è€…")
        else:
            # æ‹†åˆ†ä¸ºæ­£å¼å’Œå‚¨å¤‡
            formal_df = merged_df.iloc[:formal_count].copy()
            backup_df = merged_df.iloc[formal_count:].copy()

            # å‚¨å¤‡å¿—æ„¿è€…æŒ‰æˆç»©æ’åº - å…¼å®¹ä¸åŒç‰ˆæœ¬çš„pandas
            try:
                backup_df = backup_df.sort_values(by=score_column, ascending=False, na_last=True)
            except TypeError:
                backup_df = backup_df.fillna({score_column: -1})
                backup_df = backup_df.sort_values(by=score_column, ascending=False)

        # ç§»é™¤æˆç»©åˆ—ï¼ˆä¸éœ€è¦åœ¨è¾“å‡ºæ–‡ä»¶ä¸­æ˜¾ç¤ºï¼‰
        formal_df = formal_df.drop(columns=[score_column])
        backup_df = backup_df.drop(columns=[score_column])

        self.logger.info(f"æ‹†åˆ†å®Œæˆï¼šæ­£å¼å¿—æ„¿è€… {len(formal_df)} äººï¼Œå‚¨å¤‡å¿—æ„¿è€… {len(backup_df)} äºº")

        return formal_df, backup_df

    def _save_formal_volunteers(self, formal_df: pd.DataFrame) -> str:
        """ä¿å­˜æ­£å¼æ™®é€šå¿—æ„¿è€…è¡¨"""
        output_file = os.path.join(self.scheduling_prep_dir, CONFIG.get('files.formal_normal_volunteers'))
        self.handler.write_excel(formal_df, output_file)
        self.logger.info(f"æ­£å¼æ™®é€šå¿—æ„¿è€…è¡¨å·²ä¿å­˜åˆ°: {output_file}")
        return output_file

    def _save_backup_volunteers(self, backup_df: pd.DataFrame) -> str:
        """ä¿å­˜å‚¨å¤‡å¿—æ„¿è€…è¡¨"""
        output_file = os.path.join(self.scheduling_prep_dir, CONFIG.get('files.backup_volunteers'))
        self.handler.write_excel(backup_df, output_file)
        self.logger.info(f"å‚¨å¤‡å¿—æ„¿è€…è¡¨å·²ä¿å­˜åˆ°: {output_file}")
        return output_file

    def _update_metadata(self, metadata: Dict, formal_count: int, backup_count: int):
        """æ›´æ–°å…ƒæ•°æ®ä¸­çš„å¿—æ„¿è€…æ•°é‡ç»Ÿè®¡"""
        try:
            if 'statistics' not in metadata:
                metadata['statistics'] = {}

            metadata['statistics']['formal_normal_count'] = formal_count
            metadata['statistics']['backup_volunteer_count'] = backup_count

            # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
            metadata_file = os.path.join(self.scheduling_prep_dir, CONFIG.get('files.metadata'))
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.logger.info("å…ƒæ•°æ®å·²æ›´æ–°")

        except Exception as e:
            self.logger.error(f"æ›´æ–°å…ƒæ•°æ®å¤±è´¥: {str(e)}")

    def validate_split_result(self, formal_df: pd.DataFrame, backup_df: pd.DataFrame,
                             original_df: pd.DataFrame) -> bool:
        """éªŒè¯æ‹†åˆ†ç»“æœ"""
        try:
            # æ£€æŸ¥æ€»äººæ•°
            total_split = len(formal_df) + len(backup_df)
            if total_split != len(original_df):
                self.logger.error(f"äººæ•°ä¸åŒ¹é…ï¼šæ‹†åˆ†å {total_split}ï¼ŒåŸå§‹ {len(original_df)}")
                return False

            # æ£€æŸ¥å­¦å·é‡å¤
            all_student_ids = list(formal_df['å­¦å·']) + list(backup_df['å­¦å·'])
            original_student_ids = list(original_df['å­¦å·'])

            if set(all_student_ids) != set(original_student_ids):
                self.logger.error("å­¦å·ä¸åŒ¹é…")
                return False

            # æ£€æŸ¥æ­£å¼å¿—æ„¿è€…å’Œå‚¨å¤‡å¿—æ„¿è€…æ˜¯å¦æœ‰é‡å 
            formal_ids = set(formal_df['å­¦å·'])
            backup_ids = set(backup_df['å­¦å·'])

            if formal_ids & backup_ids:
                self.logger.error("æ­£å¼å¿—æ„¿è€…å’Œå‚¨å¤‡å¿—æ„¿è€…æœ‰é‡å ")
                return False

            self.logger.info("æ‹†åˆ†ç»“æœéªŒè¯é€šè¿‡")
            return True

        except Exception as e:
            self.logger.error(f"éªŒè¯æ‹†åˆ†ç»“æœå¤±è´¥: {str(e)}")
            return False


def main():
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='æ­£å¼æ™®é€šå¿—æ„¿è€…å’Œå‚¨å¤‡å¿—æ„¿è€…æ‹†åˆ†ç¨‹åº')
    parser.add_argument('--formal-count', type=int, help='æ­£å¼å¿—æ„¿è€…äººæ•°ï¼ˆè¦†ç›–å…ƒæ•°æ®ä¸­çš„é…ç½®ï¼‰')
    parser.add_argument('--input-dir', help='è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•è·¯å¾„')

    args = parser.parse_args()

    logger = get_logger(__file__)
    logger.info("å¼€å§‹æ‰§è¡Œæ­£å¼æ™®é€šå¿—æ„¿è€…å’Œå‚¨å¤‡å¿—æ„¿è€…æ‹†åˆ†ç¨‹åº")

    try:
        splitter = VolunteerSplitter()

        # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰è·¯å¾„ï¼Œæ›´æ–°é…ç½®
        if args.input_dir:
            splitter.input_dir = args.input_dir
        if args.output_dir:
            splitter.scheduling_prep_dir = args.output_dir

        # æ‰§è¡Œæ‹†åˆ†
        results = splitter.run_split()

        # è¾“å‡ºç»“æœ
        if not results['errors']:
            print(f"\nâœ… æ‹†åˆ†å®Œæˆï¼")
            print(f"ğŸ“Š æ­£å¼å¿—æ„¿è€…: {results['formal_count']} äºº")
            print(f"ğŸ“Š å‚¨å¤‡å¿—æ„¿è€…: {results['backup_count']} äºº")
            print(f"ğŸ“„ æ­£å¼å¿—æ„¿è€…è¡¨: {results['formal_volunteers_file']}")
            print(f"ğŸ“„ å‚¨å¤‡å¿—æ„¿è€…è¡¨: {results['backup_volunteers_file']}")
        else:
            print(f"\nâŒ æ‹†åˆ†å¤±è´¥:")
            for error in results['errors']:
                print(f"  - {error}")

    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    main()